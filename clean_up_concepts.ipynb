{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Clean-Up\n",
    "\n",
    "The concepts extracted by MetaMap contain many spurious concepts, often identified from very common words. This step produces a condensed dataframe with just the concepts whose trigger words appear relatively *rarely* in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import lemmatize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths\n",
    "\n",
    "To begin, update the paths below to the input and output directories on your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_path = \"/path/to/thread_annotated_tweets.csv\"\n",
    "concepts_dir = \"/path/to/concepts\"\n",
    "word_counts_dir = \"/path/to/word_counts\" # should contain relevant_word_counts.pkl and irrelevant_word_counts.pkl\n",
    "\n",
    "output_dir = \"/path/to/output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweet CSV file\n",
    "tweets_path = os.path.join(base_dir, \"level_0\", \"tweets.csv\")\n",
    "tweets = pd.read_csv(tweets_path, dtype=utils.dtype_spec, lineterminator='\\n')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a sample of some concepts from the concept directory\n",
    "test_concepts = pd.read_csv(os.path.join(concepts_dir, \"concepts_0.csv\"))\n",
    "test_concepts.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doctor vs. Non-Doctor Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base level\n",
    "with open(os.path.join(word_counts_dir, \"relevant_word_counts.pkl\"), \"rb\") as file:\n",
    "    doctor_info = pickle.load(file)\n",
    "\n",
    "with open(os.path.join(word_counts_dir, \"irrelevant_word_counts.pkl\"), \"rb\") as file:\n",
    "    non_doctor_info = pickle.load(file)\n",
    "\n",
    "doctor_tweet_count = doctor_info[\"tweet_count\"]\n",
    "doctor_word_counts = doctor_info[\"word_counts\"]\n",
    "non_doctor_tweet_count = non_doctor_info[\"tweet_count\"]\n",
    "non_doctor_word_counts = non_doctor_info[\"word_counts\"]\n",
    "\n",
    "relevance = {}\n",
    "for n, word_count_set in enumerate(doctor_word_counts):\n",
    "    for word, f in word_count_set.items():\n",
    "        if word.lower() in utils.FILTER_WORDS:\n",
    "            continue\n",
    "        non_doctor_f = non_doctor_word_counts[n].get(word, 0)\n",
    "        relevance[word] = (f / doctor_tweet_count + 1e-3) / (non_doctor_f / non_doctor_tweet_count + 1e-3)\n",
    "\n",
    "def concept_enrichment(concept_row):\n",
    "    words = re.split(r\"\\W\", concept_row.trigger_word)\n",
    "    trigger = \" \".join(words)\n",
    "    if trigger in relevance:\n",
    "        return (relevance.get(trigger, 0))\n",
    "    word_rels = [relevance[word] for word in words if word in relevance]\n",
    "    return np.mean(word_rels) if word_rels else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = None\n",
    "batch_index = 0\n",
    "path = os.path.join(concepts_dir, \"concepts_{}.csv\".format(batch_index))\n",
    "\n",
    "while os.path.exists(path):\n",
    "    sub_df = pd.read_csv(path)\n",
    "    print(\"Processing {}, {} concepts so far\".format(path, len(df) if df is not None else 0))\n",
    "\n",
    "    filtered_concepts = sub_df[~pd.isna(sub_df.trigger)]\n",
    "\n",
    "    # Extract the trigger word\n",
    "    filtered_concepts[\"trigger_word\"] = filtered_concepts[\"trigger\"].str.extract(r\"\\d-\\\"([^\\\"]+)\\\"-\")[0].str.lower()\n",
    "    filtered_concepts = filtered_concepts[~pd.isna(filtered_concepts.trigger_word)]    \n",
    "\n",
    "    # Filter by semtype and exclude certain words\n",
    "    filtered_concepts = utils.filter_useful_concepts(filtered_concepts)\n",
    "\n",
    "    # Compute enrichment\n",
    "    filtered_concepts[\"enrichment\"] = filtered_concepts.apply(concept_enrichment, axis=1)\n",
    "\n",
    "    # Filter for only concepts that are MORE enriched in doctor tweets than non-doctor tweets\n",
    "    filtered_concepts = filtered_concepts[filtered_concepts[\"enrichment\"] >= 1.0]\n",
    "\n",
    "    # Concatenate concepts\n",
    "    if df is None:\n",
    "        df = filtered_concepts\n",
    "    else:\n",
    "        df = pd.concat([df, filtered_concepts])\n",
    "\n",
    "    batch_index += 1\n",
    "    path = os.path.join(concepts_dir, \"concepts_{}.csv\".format(batch_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_concepts = df.drop_duplicates('preferred_name').sort_values('enrichment', ascending=False)\n",
    "print(\"Most enriched:\", unique_concepts.head(20)[['trigger_word', 'preferred_name', 'enrichment']])\n",
    "print(\"Least enriched:\", unique_concepts.tail(20)[['trigger_word', 'preferred_name', 'enrichment']])\n",
    "plt.hist(unique_concepts.enrichment, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_concepts[(unique_concepts.enrichment >= 1.0)].trigger_word.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(output_dir, \"concepts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
