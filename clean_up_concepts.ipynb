{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Clean-Up\n",
    "\n",
    "The concepts extracted by MetaMap contain many spurious concepts, often identified from very common words. This step produces a condensed dataframe with just the concepts whose trigger words appear relatively *rarely* in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import lemmatize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths\n",
    "\n",
    "To begin, update the paths below to the input and output directories on your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_path = \"/path/to/thread_annotated_tweets.csv\"\n",
    "concepts_dir = \"/path/to/concepts\"\n",
    "\n",
    "output_dir = \"intermediate_data\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweet CSV file\n",
    "tweets = pd.read_csv(tweets_path, dtype=utils.dtype_spec, lineterminator='\\n')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a sample of some concepts from the concept directory\n",
    "test_concepts = pd.read_csv(os.path.join(concepts_dir, \"concepts_0.csv\"))\n",
    "test_concepts.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Word Frequency\n",
    "\n",
    "The frequencies of English words are derived from the Google Web Trillion Word Corpus, and provided in a [unigram frequencies TSV file](http://norvig.com/ngrams/count_1w.txt) by Peter Norvig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the English language frequencies file\n",
    "english_freqs_path = \"english_language_frequencies.tsv\"\n",
    "\n",
    "if not os.path.exists(english_freqs_path):\n",
    "    print(\"Downloading English language frequencies...\")\n",
    "    resp = requests.get(\"http://norvig.com/ngrams/count_1w.txt\")\n",
    "    with open(english_freqs_path, \"wb\") as file:\n",
    "        file.write(resp.content)\n",
    "    print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.read_csv(english_freqs_path, delimiter='\\t', header=None)\n",
    "word_counts.columns = ['word', 'freq']\n",
    "word_counts = word_counts.set_index('word')\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_cache = {}\n",
    "\n",
    "def trigger_relevance(row):\n",
    "    \"\"\"\n",
    "    Computes the relevance of the trigger_word column of the given row. Relevance\n",
    "    is the average of the negative log frequencies of each word in the word_counts\n",
    "    dataframe. Words that are not present in the word_counts dataframe are given\n",
    "    a count of 1.\n",
    "    \"\"\"\n",
    "    global relevance_cache\n",
    "    \n",
    "    trigger = row[\"trigger_word\"]\n",
    "    if trigger not in relevance_cache:\n",
    "        components = re.split(r\"\\W\", trigger)\n",
    "        relevance_cache[trigger] = np.mean([-np.log(word_counts.freq.get(comp, 1)) for comp in components])\n",
    "\n",
    "    return relevance_cache[trigger]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance Cutoff\n",
    "\n",
    "What threshold should we choose for relevance? Let's take a look at some example concepts and what their trigger word relevances are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_concepts = test_concepts[~pd.isna(test_concepts.trigger)]\n",
    "\n",
    "# Trigger words are stored in a hyphen-delimited format with the fourth component \n",
    "# corresponding to the actual trigger. For example: [\"Test\"-tx-1-\"test\"-noun-0]\n",
    "filtered_concepts[\"trigger_word\"] = filtered_concepts[\"trigger\"].str.extract(r\"\\d-\\\"([^\\\"]+)\\\"-\")[0].str.lower()\n",
    "filtered_concepts = filtered_concepts[~pd.isna(filtered_concepts.trigger_word)]\n",
    "\n",
    "filtered_concepts.trigger_word.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute relevance as the negative log of the word frequency of the trigger word.\n",
    "unique_concepts = filtered_concepts.drop_duplicates('trigger_word')\n",
    "print(\"Computing relevance for {} concepts...\".format(len(unique_concepts)))\n",
    "unique_concepts[\"relevance\"] = unique_concepts.apply(trigger_relevance, axis=1)\n",
    "print(\"Sorting...\")\n",
    "unique_concepts = unique_concepts.sort_values(by='relevance', ascending=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample of these concepts and their relevances\n",
    "plt.figure()\n",
    "plt.hist(unique_concepts['relevance'], bins=np.arange(-24, 1))\n",
    "plt.xlabel(\"Relevance\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Sample the concepts in each relevance range and print a few\n",
    "for relevance_range in [(-25, -20), (-20, -15), (-15, -13), (-13, -10), (-10, -1), (-1, 1)]:\n",
    "    print(\"Concept triggers with relevances between {} and {}:\".format(*relevance_range))\n",
    "    sample = unique_concepts[(unique_concepts.relevance >= relevance_range[0]) & \n",
    "                             (unique_concepts.relevance < relevance_range[1])].sample(n=5)\n",
    "    print(sample[['preferred_name', 'trigger_word', 'relevance']])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the most relevant concepts seem to begin occurring at a relevance of around -13. We therefore chose the cutoff for our concept relevance to be -13.\n",
    "\n",
    "The next cell loads all the concepts from each batch, filters them by relevance, and adds them to an overall dataframe. *Note:* This cell can take a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's write a condensed DF with relevances only above the given threshold\n",
    "relevance_threshold = -13.0\n",
    "\n",
    "df = None\n",
    "batch_index = 0\n",
    "path = os.path.join(concepts_dir, \"concepts_{}.csv\".format(batch_index))\n",
    "\n",
    "while os.path.exists(path):\n",
    "    sub_df = pd.read_csv(path)\n",
    "    print(\"Processing {}, {} concepts so far\".format(path, len(df) if df is not None else 0))\n",
    "    \n",
    "    filtered_concepts = sub_df[~pd.isna(sub_df.trigger)]\n",
    "    filtered_concepts[\"trigger_word\"] = filtered_concepts[\"trigger\"].str.extract(r\"\\d-\\\"([^\\\"]+)\\\"-\")[0].str.lower()\n",
    "    filtered_concepts = filtered_concepts[~pd.isna(filtered_concepts.trigger_word)]    \n",
    "    filtered_concepts[\"relevance\"] = filtered_concepts.apply(trigger_relevance, axis=1)\n",
    "    filtered_concepts = filtered_concepts[filtered_concepts[\"relevance\"] >= relevance_threshold]\n",
    "    \n",
    "    # Concatenate concepts\n",
    "    if df is None:\n",
    "        df = filtered_concepts\n",
    "    else:\n",
    "        df = pd.concat([df, filtered_concepts])\n",
    "        \n",
    "    batch_index += 1\n",
    "    path = os.path.join(concepts_dir, \"concepts_{}.csv\".format(batch_index))\n",
    "\n",
    "# Drop cases where the same concept is extracted multiple times from the same tweet\n",
    "df = df.drop_duplicates(subset=[\"tweet_id\", \"cui\"])\n",
    "df.to_csv(os.path.join(output_dir, \"all_concepts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did we do?\n",
    "print(\"Extracted {} concepts with {} unique trigger words/phrases.\".format(len(df), len(df.trigger_word.unique())))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(unique_concepts['relevance'], bins=np.arange(-24, 1))\n",
    "plt.xlabel(\"Relevance\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Relevance Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Look at a random sample of concepts\n",
    "df.drop_duplicates('cui').sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
