{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Thread Augmentation\n",
    "\n",
    "Many tweets are part of **threads**, which consist of multiple tweets in a linked-list sequence of replies to one another. Since some of the tweets may not have contained the original coronavirus keywords, this step pulls tweets in threads for which at least one tweet is in the dataset. This consists of three steps:\n",
    "\n",
    "1. Extract **upstream tweets**, which are explicitly linked to in the `in_reply_to_status_id_str` field of the tweet. We can do this using a simple hydrate command with Twarc.\n",
    "2. Extract **downstream tweets**, which are not explicitly linked. Instead, we look for tweets in each user's timeline within a two-day window on either side of their tweets in the dataset, and recursively find the tweets that link back to the original dataset.\n",
    "3. Join these two sets together with the original tweet dataset, and assign each tweet a **thread ID**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6608,
     "status": "ok",
     "timestamp": 1591758108595,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "SQUjbWGCL1eD",
    "outputId": "bd149b48-5409-429f-e3ca-e68dd4717452"
   },
   "outputs": [],
   "source": [
    "import twarc\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths\n",
    "\n",
    "Input the paths to your Twarc credentials, and input and output paths below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Twarc credentials path (usually expanded version of ~/.twarc)\n",
    "credentials_path = \"/Users/venkatesh-sivaraman/.twarc\"\n",
    "\n",
    "# Path to CSV file batches\n",
    "input_dir = \"/path/to/directory/containing/csvs\"\n",
    "\n",
    "# Path to scratch directory for intermediate results\n",
    "intermediate_dir = \"../data/intermediates\"\n",
    "if not os.path.exists(intermediate_dir):\n",
    "    os.mkdir(intermediate_dir)\n",
    "    \n",
    "# Path to output directory\n",
    "output_dir = \"../data/tweets\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19522,
     "status": "ok",
     "timestamp": 1591663290062,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "kMXLYCPy3fS-",
    "outputId": "c5398ac5-cc38-442e-8756-895c5a4b212d"
   },
   "outputs": [],
   "source": [
    "# Load input tweets\n",
    "filenames = [os.path.join(input_dir, path) for path in os.listdir(input_dir)\n",
    "                 if path.endswith(\".csv\") and not path.startswith(\".\")]\n",
    "df = pd.concat([pd.read_csv(filename, dtype=utils.dtype_spec, lineterminator='\\n')\n",
    "                for filename in filenames])\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')].reset_index(drop=True)\n",
    "\n",
    "print(len(df), \"tweets\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(intermediate_dir, \"initial_doctor_tweets.csv\"), line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58KpiRbtZUUW"
   },
   "outputs": [],
   "source": [
    "# Load Twarc object\n",
    "t = utils.load_twarc(credentials_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MchFjBBGvTcL"
   },
   "source": [
    "# Upstream Tweets\n",
    "\n",
    "We want to extract specifically all messages that were replied to by a tweet in the dataset, or that reply to a tweet in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1591663399329,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "jyXWcY2lwpb3",
    "outputId": "3a2b277b-dd0a-4dcc-b2c3-a423db2687b1"
   },
   "outputs": [],
   "source": [
    "reply_ids = df[~pd.isna(df.reply_to_id) & (df.reply_to_user == df.user_id)].reply_to_id.unique().tolist()\n",
    "print(\"{} reply IDs\".format(len(reply_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 437126,
     "status": "ok",
     "timestamp": 1591663893108,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "qTwEBgrfwbfI",
    "outputId": "03aac3e9-e575-4f85-e797-83eefde79226",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recursively extract replies\n",
    "seen_ids = set()\n",
    "reply_ids = list(set(reply_ids))\n",
    "hydrated_replies = []\n",
    "i = 0\n",
    "\n",
    "while reply_ids:\n",
    "    print(\"Round {}, {} tweets to hydrate\".format(i, len(reply_ids)))\n",
    "    new_replies = list(t.hydrate(reply_ids))\n",
    "    hydrated_replies += new_replies\n",
    "    seen_ids |= set([tweet[\"id_str\"] for tweet in new_replies])\n",
    "    # Mark tweets that are in reply to a message by the same user for the next round\n",
    "    reply_ids = [tweet[\"in_reply_to_status_id_str\"] for tweet in new_replies\n",
    "                 if tweet[\"in_reply_to_status_id_str\"] is not None and\n",
    "                 tweet[\"in_reply_to_status_id_str\"] not in seen_ids and\n",
    "                 tweet[\"in_reply_to_user_id_str\"] == tweet[\"user\"][\"id_str\"]]\n",
    "    i += 1\n",
    "\n",
    "# Write upstream tweets as JSON\n",
    "print(\"Writing JSON...\")\n",
    "upstream_tweets = []\n",
    "with open(os.path.join(intermediate_dir, \"all_upstream_tweets.json\"), \"w\") as file:\n",
    "    for item in hydrated_replies:\n",
    "        tweet = json.dumps(item)\n",
    "        file.write(tweet + \"\\n\")\n",
    "        upstream_tweets.append(utils.json_to_tweet(item))\n",
    "\n",
    "print(\"Writing CSV...\")\n",
    "upstream_df = pd.DataFrame(upstream_tweets)\n",
    "upstream_df.to_csv(os.path.join(intermediate_dir, \"all_upstream_tweets.csv\"),\n",
    "                   line_terminator='\\n')\n",
    "print(\"Wrote {} upstream tweets.\".format(len(hydrated_replies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v75HK3IWx5gk"
   },
   "source": [
    "# Downstream Tweets\n",
    "\n",
    "Next use user timelines to find tweets that reply to tweets in the dataset. To do this efficiently, we find a unique set of users and find a consensus date window in which to search for tweets. We assume that replies occur within two days of the original tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 192329,
     "status": "ok",
     "timestamp": 1591664198158,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "pMBNe7zrxgQl",
    "outputId": "7f5138cf-154e-4eca-d8e9-260a0b1dd433"
   },
   "outputs": [],
   "source": [
    "# First establish a list of reference IDs for each date\n",
    "def get_date(tweet, day_delta=0):\n",
    "    date = datetime.datetime.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "    if day_delta != 0:\n",
    "        date = date + datetime.timedelta(days=day_delta)\n",
    "    return datetime.date.strftime(date, '%Y-%m-%d')\n",
    "\n",
    "min_ids = {}\n",
    "max_ids = {}\n",
    "for i, tweet in df.iterrows():\n",
    "    if i % 100000 == 0:\n",
    "        print(i)\n",
    "    id_num = int(tweet[\"id\"])\n",
    "    date = get_date(tweet)\n",
    "    min_ids[date] = min(min_ids.get(date, 1e30), id_num)\n",
    "    max_ids[date] = max(max_ids.get(date, 0), id_num)\n",
    "\n",
    "print(sorted(min_ids.items())[-5:], sorted(max_ids.items())[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8851,
     "status": "ok",
     "timestamp": 1591664284691,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "a1fz2GFpzj9P",
    "outputId": "7df70d08-302e-4f87-bcaa-5c8a0a8cece2"
   },
   "outputs": [],
   "source": [
    "# Now get a set of users and the required search dates\n",
    "user_dates = {}\n",
    "\n",
    "tweets_with_replies = df[~pd.isna(df.reply_to_id) & (df.reply_to_user == df.user_id)]\n",
    "for i, tweet in tweets_with_replies.iterrows():\n",
    "    user = tweet[\"user_id\"]\n",
    "    min_date = get_date(tweet, -2)\n",
    "    max_date = get_date(tweet, 2)\n",
    "    if user in user_dates:\n",
    "        user_dates[user] = (min(min_date, user_dates[user][0]),\n",
    "                          min(max_date, user_dates[user][1]))\n",
    "    else:\n",
    "        user_dates[user] = (min_date, max_date)\n",
    "\n",
    "print(\"{} users\".format(len(user_dates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1591664288668,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "ehqJ3gHc2CSe",
    "outputId": "1750197e-f420-4b33-f4c5-4c14a415b0e4"
   },
   "outputs": [],
   "source": [
    "# Fill in reference IDs for dates that aren't in the set. To do this, we'll create two\n",
    "# rough, conservative linear models for tweet IDs over time by estimating the increment\n",
    "# in the minimum and maximum tweet IDs per day.\n",
    "\n",
    "# Note that we're going to give each tweet a two-day interval on either side, so the\n",
    "# exactness of this estimate isn't important except to improve the performance of the\n",
    "# tweet scraper.\n",
    "\n",
    "available_days = sorted(min_ids.keys())\n",
    "series = [available_days[0]]\n",
    "current = series[-1]\n",
    "\n",
    "min_id_items = []\n",
    "max_id_items = []\n",
    "\n",
    "date_index = 0\n",
    "while current != available_days[-1]:\n",
    "    date = datetime.datetime.strptime(current, '%Y-%m-%d')\n",
    "    current = datetime.date.strftime(date + datetime.timedelta(days=1), '%Y-%m-%d')\n",
    "    if current in min_ids:\n",
    "        min_id_items.append((date_index, min_ids[current]))\n",
    "    if current in max_ids:\n",
    "        max_id_items.append((date_index, max_ids[current]))\n",
    "    series.append(current)\n",
    "    date_index += 1\n",
    "\n",
    "min_inc_per_day = (min_id_items[-1][1] - min_id_items[0][1]) / (min_id_items[-1][0] - min_id_items[0][0])\n",
    "max_inc_per_day = (max_id_items[-1][1] - max_id_items[0][1]) / (max_id_items[-1][0] - max_id_items[0][0])\n",
    "earliest_date = datetime.datetime.strptime(available_days[0], '%Y-%m-%d')\n",
    "\n",
    "def get_min_id(date_str):\n",
    "    \"\"\"Estimates the minimum tweet ID for the given date string, in the format YYYY-MM-DD.\"\"\"\n",
    "    date = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    days = (date - earliest_date).days\n",
    "    return min_ids[available_days[0]] + days * min_inc_per_day\n",
    "\n",
    "def get_max_id(date_str):\n",
    "    \"\"\"Estimates the maximum tweet ID for the given date string, in the format YYYY-MM-DD.\"\"\"\n",
    "    date = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    days = (date - earliest_date).days\n",
    "    return max_ids[available_days[0]] + days * max_inc_per_day\n",
    "\n",
    "print(\"Estimate:\", get_min_id(\"2020-02-04\"), \"Actual:\", min_ids[\"2020-02-04\"])\n",
    "print(\"Estimate:\", get_max_id(\"2020-02-08\"), \"Actual:\", max_ids[\"2020-02-08\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15032098,
     "status": "ok",
     "timestamp": 1591679441863,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "qObjCq4u1a3Z",
    "outputId": "35b56c6e-0f72-4005-85a8-588ea0c2f80a"
   },
   "outputs": [],
   "source": [
    "current_batch = []\n",
    "current_csv = []\n",
    "batch_idx = 0\n",
    "\n",
    "user_items = sorted(user_dates.items())\n",
    "\n",
    "i = 0\n",
    "for i, (user_id, (min_date, max_date)) in enumerate(user_items):\n",
    "    if i % 100 == 0:\n",
    "        # Periodically sleep to appease the Twitter rate limiting gods\n",
    "        print(i)\n",
    "        time.sleep(20)\n",
    "    i += 1\n",
    "    \n",
    "    # Compute the boundary tweet IDs needed to search the Twitter timeline for this user\n",
    "    min_id = int(get_min_id(min_date))\n",
    "    max_id = int(get_max_id(max_date))\n",
    "    for tweet in t.timeline(user_id=user_id, max_id=max_id, since_id=min_id):\n",
    "        current_batch.append(tweet)\n",
    "        current_csv.append(utils.json_to_tweet(tweet))\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Writing\")\n",
    "        with open(os.path.join(intermediate_dir, \"timeline_tweets_{}.json\".format(batch_idx)), \"w\") as file:\n",
    "            for item in current_batch:\n",
    "                file.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "        batch_df = pd.DataFrame(current_csv)\n",
    "        batch_df.to_csv(os.path.join(intermediate_dir, \"timeline_tweets_{}.csv\".format(batch_idx)),\n",
    "                      line_terminator=\"\\n\")\n",
    "        batch_idx += 1\n",
    "        current_batch = []\n",
    "        current_csv = []\n",
    "        \n",
    "# Write out the stragglers\n",
    "print(\"Writing last batch\")\n",
    "with open(os.path.join(intermediate_dir, \"timeline_tweets_{}.json\".format(batch_idx)), \"w\") as file:\n",
    "    for item in current_batch:\n",
    "        file.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "batch_df = pd.DataFrame(current_csv)\n",
    "batch_df.to_csv(os.path.join(intermediate_dir, \"timeline_tweets_{}.csv\".format(batch_idx)), line_terminator='\\n')\n",
    "batch_idx += 1\n",
    "current_batch = []\n",
    "current_csv = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_THMO6qdStE"
   },
   "source": [
    "# Putting It All Together\n",
    "\n",
    "We want to read all the timeline tweets, as well as the original tweet sets and upstream tweets, and put together a directed acyclic graph of tweet replies. This DAG will contain many individual components that are disconnected from each other, corresponding to tweet groups by individual users. We find all tweets in reply graphs that contain at least one tweet in the original dataset, and label those as belonging to a single \"thread.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4493,
     "status": "ok",
     "timestamp": 1591712021100,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "EQKKdyd4Rdx2",
    "outputId": "97a41964-bfa4-4071-9132-cbe60a4498a0"
   },
   "outputs": [],
   "source": [
    "batch_idx = 0\n",
    "path = os.path.join(intermediate_dir, \"timeline_tweets_{}.csv\".format(batch_idx))\n",
    "timelines = None\n",
    "while os.path.exists(path):\n",
    "    print(\"Reading {}...\".format(os.path.basename(path)))\n",
    "    sub_df = pd.read_csv(path, dtype=utils.dtype_spec, lineterminator='\\n')\n",
    "    if timelines is None:\n",
    "        timelines = sub_df\n",
    "    else:\n",
    "        timelines = pd.concat([timelines, sub_df])\n",
    "    batch_idx += 1\n",
    "    path = os.path.join(intermediate_dir, \"timeline_tweets_{}.csv\".format(batch_idx))\n",
    "timelines = timelines.loc[:, ~timelines.columns.str.contains('^Unnamed')].reset_index(drop=True)    \n",
    "\n",
    "timelines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32586,
     "status": "ok",
     "timestamp": 1591714819784,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "-3n1bE2FKmSJ",
    "outputId": "5a5d298b-896b-42dc-ad02-9e063785e615"
   },
   "outputs": [],
   "source": [
    "# Let's build a set of all the threaded tweets we know about.\n",
    "\n",
    "# Uncomment to load initial tweets from a previous run\n",
    "# df = pd.read_csv(os.path.join(intermediate_dir, \"initial_doctor_tweets.csv\"), dtype=utils.dtype_spec, index_col=0, lineterminator='\\n')\n",
    "\n",
    "all_threaded_tweets = pd.concat([\n",
    "    timelines,\n",
    "    pd.read_csv(os.path.join(intermediate_dir, \"all_upstream_tweets.csv\"), dtype=utils.dtype_spec, index_col=0, lineterminator='\\n'),\n",
    "    df\n",
    "])\n",
    "\n",
    "print(\"{} tweets total\".format(len(all_threaded_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6/22/20: The old approach to threads may have been incorrect because some tweets have multiple\n",
    "# replies, so the graph is more of a DAG than a linked list. We need to find the correct tweets\n",
    "# to concatenate in these cases.\n",
    "\n",
    "downstream_replies = {}\n",
    "upstream_replies = {}\n",
    "\n",
    "dedup_tweets = all_threaded_tweets.drop_duplicates(\"id\")\n",
    "dedup_tweets[\"id_num\"] = dedup_tweets[\"id\"].astype(int)\n",
    "dedup_tweets = dedup_tweets.sort_values(\"id_num\", ascending=False).reset_index()\n",
    "print(\"Removed {} tweets\".format(len(all_threaded_tweets) - len(dedup_tweets)))\n",
    "\n",
    "tweet_ids = {row[\"id\"]: i for i, row in dedup_tweets.iterrows()}\n",
    "\n",
    "missing_tweets = 0\n",
    "for i, row in dedup_tweets.iterrows():\n",
    "    if i % 100000 == 0: print(i, len(downstream_replies), len(upstream_replies))\n",
    "    if row[\"reply_to_id\"] and row[\"reply_to_user\"] == row[\"user_id\"]:\n",
    "        if row[\"reply_to_id\"] not in tweet_ids:\n",
    "            missing_tweets += 1\n",
    "            continue\n",
    "        reply_index = tweet_ids[row[\"reply_to_id\"]]\n",
    "        downstream_replies.setdefault(reply_index, set()).add(i)\n",
    "        upstream_replies.setdefault(i, set()).add(reply_index)\n",
    "print(\"Missing tweets: {}\".format(missing_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6/22/20: Combine tweet groups containing self-replies\n",
    "tweet_groups = []\n",
    "non_linear_tweet_groups = []\n",
    "seen_tweets = set()\n",
    "\n",
    "for base_id, upstream_tweets in upstream_replies.items():\n",
    "    if base_id in seen_tweets:\n",
    "        continue\n",
    "\n",
    "    # Travel upward to the root of this tweet thread\n",
    "    assert len(upstream_tweets) == 1\n",
    "    root = list(upstream_tweets)[0]\n",
    "    while root in upstream_replies:\n",
    "        up = upstream_replies[root]\n",
    "        assert len(up) == 1\n",
    "        root = list(up)[0]\n",
    "    \n",
    "    # Grab everything downstream of this root\n",
    "    group = set()\n",
    "    queue = [root]\n",
    "    nonlinear = False\n",
    "    while queue:\n",
    "        curr = queue.pop(0)\n",
    "        assert curr not in group\n",
    "        assert curr not in seen_tweets\n",
    "        group.add(curr)\n",
    "        seen_tweets.add(curr)\n",
    "        queue += list(downstream_replies.get(curr, set()))\n",
    "        if len(downstream_replies.get(curr, set())) > 1:\n",
    "            nonlinear = True\n",
    "            \n",
    "    tweet_groups.append(group)\n",
    "    if nonlinear:\n",
    "        non_linear_tweet_groups.append(group)\n",
    "    if len(tweet_groups) % 10000 == 0:\n",
    "        print(\"{} tweet groups so far ({} nonlinear)\".format(len(tweet_groups), len(non_linear_tweet_groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: pick a tweet group and look at its structure\n",
    "\n",
    "def print_group(group):\n",
    "    root = list(group)[0]\n",
    "    while root in upstream_replies:\n",
    "        up = upstream_replies[root]\n",
    "        assert len(up) == 1\n",
    "        root = list(up)[0]\n",
    "    \n",
    "    # Grab everything downstream of this root\n",
    "    queue = [root]\n",
    "    print_ids = {}\n",
    "    while queue:\n",
    "        curr = queue.pop(0)\n",
    "        tweet = dedup_tweets.iloc[curr]\n",
    "        print(\"{}: {} said at {} in reply to {}:\".format(tweet[\"id\"], tweet[\"screen_name\"], tweet[\"created_at\"], print_ids.get(tweet[\"reply_to_id\"], -1)))\n",
    "        print(tweet[\"full_text\"])\n",
    "        if len(downstream_replies.get(curr, set())) > 1:\n",
    "            print(\"NONLINEAR\")\n",
    "        print(\"=\" * 40)\n",
    "        print_ids[tweet[\"id\"]] = len(print_ids)\n",
    "        queue += list(downstream_replies.get(curr, set()))\n",
    "    \n",
    "sample_group = non_linear_tweet_groups[np.random.choice(len(non_linear_tweet_groups))]\n",
    "print_group(sample_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally filter threaded tweets by those posted within this time interval of the tweet in the dataset.\n",
    "## To avoid arbitrary cutoffs, we currently don't use this interval.\n",
    "\n",
    "time_period = datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each tweet in the original dataframe, grab tweets in the thread within an hour of each tweet and call\n",
    "# them a thread.\n",
    "\n",
    "thread_ids = {}\n",
    "current_thread = 0\n",
    "\n",
    "df[\"id_num\"] = df[\"id\"].astype(int)\n",
    "sorted_original_tweets = df.sort_values(by=\"id_num\", ascending=True).reset_index()\n",
    "\n",
    "for i, row in sorted_original_tweets.iterrows():\n",
    "    if i % 100000 == 0:\n",
    "        print(i)\n",
    "    if row[\"id\"] in thread_ids:\n",
    "        continue\n",
    "    thread_ids[row[\"id\"]] = current_thread\n",
    "\n",
    "    if time_period:\n",
    "        timestamp = datetime.datetime.strptime(row.created_at, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "        lower_time_bound = timestamp - time_period\n",
    "        upper_time_bound = timestamp + time_period\n",
    "    \n",
    "    # Grab upstream tweets\n",
    "    curr = tweet_ids[row[\"id\"]]\n",
    "    while curr in upstream_replies:\n",
    "        curr = list(upstream_replies[curr])[0]\n",
    "        tweet = dedup_tweets.iloc[curr]\n",
    "        \n",
    "        if time_period:\n",
    "            curr_ts = datetime.datetime.strptime(tweet.created_at, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "            if curr_ts < lower_time_bound or curr_ts > upper_time_bound:\n",
    "                break\n",
    "\n",
    "        thread_ids[tweet[\"id\"]] = current_thread\n",
    "    \n",
    "    # Downstream tweets\n",
    "    queue = [tweet_ids[row[\"id\"]]]\n",
    "    while queue:\n",
    "        curr = queue.pop(0)\n",
    "        if curr not in downstream_replies:\n",
    "            continue\n",
    "        tweet = dedup_tweets.iloc[curr]\n",
    "        \n",
    "        if time_period:\n",
    "            curr_ts = datetime.datetime.strptime(tweet.created_at, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "            if curr_ts < lower_time_bound or curr_ts > upper_time_bound:\n",
    "                continue\n",
    "\n",
    "        thread_ids[tweet[\"id\"]] = current_thread\n",
    "        \n",
    "        queue += list(downstream_replies[curr])\n",
    "\n",
    "    current_thread += 1\n",
    "    \n",
    "print(\"{} thread IDs\".format(len(thread_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_tweets = dedup_tweets[dedup_tweets.id.isin(thread_ids)]\n",
    "present_tweets[\"thread_id\"] = present_tweets.id.map(lambda id: thread_ids[id])\n",
    "present_tweets = present_tweets.sort_values(by=\"id\", ascending=True)\n",
    "present_tweets = present_tweets.loc[:, ~present_tweets.columns.str.contains('^Unnamed|^index$|^level_')].reset_index(drop=True)    \n",
    "present_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_tweets.to_csv(os.path.join(output_dir, \"thread_annotated_tweets.csv\"),\n",
    "                 line_terminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1582,
     "status": "ok",
     "timestamp": 1591716019238,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "a9PZnY2RNcX4",
    "outputId": "c841cc5d-8c15-4ca6-b680-d7a4a76a15c3"
   },
   "outputs": [],
   "source": [
    "# Find some example threads by screen name\n",
    "test_screen_name = \"AdamJKucharski\"\n",
    "\n",
    "def print_thread_groupby(thread):\n",
    "    for i, tweet in thread.iterrows():\n",
    "        print(tweet.id, tweet.full_text)\n",
    "    print(\"===\")\n",
    "    print(\"\")\n",
    "\n",
    "screen_name_tweets = present_tweets[present_tweets.screen_name == test_screen_name]\n",
    "screen_name_tweets.groupby(['thread_id']).apply(print_thread_groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1194,
     "status": "ok",
     "timestamp": 1591716209381,
     "user": {
      "displayName": "Venkatesh Sivaraman",
      "photoUrl": "",
      "userId": "08037131062068375405"
     },
     "user_tz": 240
    },
    "id": "qRIjSjjRWHr6",
    "outputId": "1bc77321-61d9-4780-8236-581098ce0b59"
   },
   "outputs": [],
   "source": [
    "# Sanity check for if the threads are being extracted correctly:\n",
    "# Many tweet threads contain pagination markers, like 1/5, 2/5, etc.\n",
    "# Here we extract tweets that contain the ending pagination marker, i.e. the numerator and denominator\n",
    "# are the same value, and look at the threads that contain those tweets. We should see the full numerical\n",
    "# range of indexes from 1 to n for each thread.\n",
    "\n",
    "thread_flags = present_tweets[present_tweets.full_text.str.contains(r\"(\\d+)/\\1\\b\")]\n",
    "for i, start_tweet in thread_flags.iloc[50:60].iterrows():\n",
    "    thread_group = present_tweets[present_tweets.thread_id == start_tweet.thread_id]\n",
    "    for j, tweet in thread_group.iterrows():\n",
    "        print(\"{}: {} said at {} in reply to {}:\".format(tweet[\"id\"], tweet[\"screen_name\"], tweet[\"created_at\"], tweet[\"reply_to_id\"]))\n",
    "        print(tweet[\"full_text\"])\n",
    "        print(\"=\" * 40)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tweet_thread_augmentation.ipynb",
   "provenance": [
    {
     "file_id": "14YViT2hopL-40ogFE5YskZlzWbnvGO41",
     "timestamp": 1588452728320
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
